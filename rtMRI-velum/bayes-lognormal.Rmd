---
title: "Bayesian models (log-normal)"
output: 
  html_document: 
    highlight: tango
    number_sections: yes
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = here::here())
library(tidyverse)
theme_set(theme_minimal())
library(brms)
library(bayesplot)
library(tidybayes)
library(broom.mixed)
options(mc.cores = parallel::detectCores())
my.seed <- 123
set.seed(my.seed)
```

# Read data

```{r read-data}
matdat <- read.csv('./rtMRI-velum/velum_data.csv', header = T)
matdat$word <- paste0(matdat$prev, matdat$vowel)

# separate coda contexts by alveolar voiced stop vs. alveolar voiceless stop
voiceless <- c('nt__','nt_@','nt_6','nt_a')
voiced    <- c('nd_@','nd_6','nd_a')

# only include alveolar nasal items preceding a voiced or voiceless stop consonant
coda <- c(voiceless,voiced)

# only include neutrally stressed utterances
stresses <- c("N")

# subset the data
subdat <- matdat[matdat$post %in% coda & matdat$stress %in% stresses, ]
subdat$voicing <- c()
subdat$voicing[subdat$post %in% voiceless] <- "voiceless"
subdat$voicing[subdat$post %in% voiced] <- "voiced"
subdat <- subdat %>%
  # Centre speech rate (no need to scale it, since that's taken care of by
  # random intercept).
  mutate(speech_rate_c = sp_rate - mean(sp_rate))
```

# Velum gesture duration

## Model specification

Testing the hypothesis that the duration of the velum gesture is not affected by `voicing`.

We can use a log-normal distribution as the likelihood, since speech segment durations generally tend to be right-skewed (Rosen 2005Í¾ Ratnikova 2017; Gahl & Baayen 2019).

Speech rate is always negatively correlated with speech segment durations, so it is better to include it.
Plug 2018 recommends the formula `number of syllables / utterance duration`.

We include the following predictors:

- C2 voicing: voiced vs voiceless.
- Centred speech rate (continuous).
- By-speaker and by-"word" varying intercept.
- Varying slopes for voicing and centred speech rate for both varying intercepts.

## Priors

```{r m1-ln-dv}
# create the dependent variable
subdat$DV <- subdat$velumopening_maxvel_dur * 1000
```

Let's get which priors we have to set.

```{r m1-ln-prior-df}
m1_ln_prior_df <- get_prior(
  DV ~
    voicing +
    speech_rate_c +
    (voicing + speech_rate_c|speaker) +
    (voicing + speech_rate_c|word),
  data = subdat,
  family = lognormal
)
```

This model requires 17 parameters to be estimated.

We set the following priors:

- For the Intercept (`voicing = voiced`, `speech_rate_c = 0`), `Normal(0, 3)`. This corresponds to the prior belief that the intercept is between approximately 0 and 400 ms at 95% confidence (weakly informative prior).
- For voicing (`voicing = voiceless`), `Normal(0, 1)`. Thiss corresponds to the prior belief that the effect of voicing is between 0.13 and 7.4 in odds at 95% confidence (weakly informative prior).
- The same prior (`Normal(0, 1)`) for centred speech rate.
- A `HalfCauchy(0, 0.1)` prior for standard deviation of the random effects (`sd`) and the residual standard deviation (`sigma`). This corresponds to a prior belief that the standard deviations are between 0 and 2.55 in log-odds (= 13 in odds) at 95% confidence. This is a very weakly informative (almost uniform) prior, which is compatible with recommendations by Gelmann.
- An `LKJ(2)` prior for the correlations in the random effect, as recommended by several authors.

```{r m1-ln-priors}
m1_ln_priors <- c(
  prior(normal(0, 3), class = Intercept),
  prior(normal(0, 1), class = b, coef = voicingvoiceless),
  prior(normal(0, 1), class = b, coef = speech_rate_c),
  prior(cauchy(0, 0.1), class = sd),
  prior(cauchy(0, 0.1), class = sigma),
  prior(lkj(2), class = cor)
)
```

We can now run prior predictive checks with `brms(..., sample_prior = "only")`.

```{r m1-ln-prior-checks}
m1_ln_prior_check <- brm(
  DV ~
    voicing +
    speech_rate_c +
    (voicing + speech_rate_c|speaker) +
    (voicing + speech_rate_c|word),
  data = subdat,
  family = lognormal,
  prior = m1_ln_priors,
  sample_prior = "only",
  file = "./rtMRI-velum/models/m1_ln_prior_check"
)
```

These are the marginal posteriors based on the priors. The priors convey intercepts and effects of up to about 1 s (weakly informative).

```{r m1-ln-prior-cond}
plot(conditional_effects(m1_ln_prior_check), ask = FALSE)
```

## Fit model

```{r m1-ln}
m1_ln <- brm(
  DV ~
    voicing +
    speech_rate_c +
    (voicing + speech_rate_c|speaker) +
    (voicing + speech_rate_c|word),
  data = subdat,
  family = lognormal,
  prior = m1_ln_priors,
  file = "./rtMRI-velum/models/m1_ln",
  control = list(adapt_delta = 0.99)
)
```

## Model checks

### Model plots

The chain plots look good.

```{r m1-ln-plot}
plot(m1_ln, ask = FALSE)
```

### Posterior predictive checks

Let's have a look at the posterior predictive plot. If the dark blue line and the light blue lines overlap, it is good.

```{r m1-ln-pp}
pp_check(m1_ln, nsamples = 100)
```

The posteriors are better than when the Gaussian was used as likelihood (especially on the right tail), but still not great. I suspect we are missing important predictors, probably vowel.

### Sensitivity

If posterior *z*-scores (`z`) are low, and posterior shrinkage (`s`) is 1, it's good. In theory, points in the plot should concentrate in the bottom right corner (this means ideal fit).
When they are in the top right corner, it means the model is overfitting (the data dominates).
When they are in the bottom left corner, it means the priors dominate and the data does not add information (which is bad).
If there are in the top left corner, this indicates a conflict between the priors and the posteriors (which is again bad).

```{r m1-ln-fixed}
m1_ln_fixed <- tidy(m1_ln, effects = "fixed", conf.level = 0.95, fix.intercept = FALSE)
```

```{r m1-ln-sensitivity, warning=FALSE}
m1_ln_fixed %>%
  mutate(
    theta = c(0, 0, 0),
    sigma_prior = c(3, 1, 1),
    # it's called here std.error but is the standard deviation
    z = abs((estimate - theta) / std.error),
    s = 1 - (std.error^2 / sigma_prior^2)
  ) %>%
  ggplot(aes(s, z, label = term)) +
  geom_point() +
  geom_label(nudge_x = -0.1) +
  xlim(0, 1) + ylim(0, 5)
```

We are overfitting `voicing`, but it is not surprising since the priors are very wide.
This should not be a concern for us (as long as the priors don't dominate).

## Posteriors

Since the posterior checks are more or less fine ($\hat{R}$ are good too), we can have a look at the posteriors.

```{r m1-ln-summary}
m1_ln
```

Let's plot the posteriors of the effect of voicing and speech rate (in log-odds).

```{r m1-ln-areas}
mcmc_areas(m1_ln, regex_pars = "^b_[vs]")
```

Voicing (= voiceless) has a clear negative effect on velum gesture duration.
The effect of speech rate is so small (very slightly negative) that is practically null, with some degree of uncertainty.

We can plot the marginal posteriors.

```{r m1-ln-cond-voi}
conditional_effects(m1_ln, effects = "voicing")
```

```{r m1-ln-cond-sr}
conditional_effects(m1_ln, effects = "speech_rate_c", spaghetti = TRUE, nsamples = 100)
```

## Minimal reporting

The mean duration of the velum gesture when C2 is voiced was estimated to be between `r round(exp(4.97))` and `r round(exp(5.07))` ms at 95% confidence (95% CI in log-odds [4.97, 5.07], posterior mean $\hat{\theta}$ = 5.02, SE = 0.03).

When C2 is voiceless, the velum gesture changes by a factor of `r round(exp(-0.14), 2)`-`r round(exp(-0.05), 2)` at 95% confidence (95% CI in log-odds [-0.14, -0.05], posterior mean $\hat{\theta}$ = -0.1, SE = 0.02).
Odds below 1 indicate a decrease, and at the posterior mean of the intercept (180 ms) these odds indicate a decrease between 9 and 23.5 ms.

The effect of (centred) speech rate is between `r round(exp(-0.03), 2)`-`r round(exp(0.02), 2)` at 95% confidence. At the posterior mean of the intercept (180 ms) these odds indicate a change between -5 and +4 ms for each unit increase of speech rate.
